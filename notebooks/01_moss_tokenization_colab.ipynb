{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MOSS Tokenization + Retrieval Training in Colab\n",
        "\n",
        "This notebook covers full baseline pipeline for the project in one place.\n",
        "\n",
        "Pipeline in this notebook:\n",
        "1. Clone project repo\n",
        "2. Install dependencies\n",
        "3. Prepare audio corpus (upload / Google Drive / HuggingFace dataset / custom mix)\n",
        "4. Run tokenization (`audio -> tokens`)\n",
        "5. Validate output token files\n",
        "6. Build train/val/test split files\n",
        "7. Train baseline contrastive embedder\n",
        "8. Evaluate `Recall@1/10/100`, `MRR`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Runtime\n",
        "\n",
        "In Colab: `Runtime -> Change runtime type -> GPU` (optional but recommended)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform\n",
        "print('Python:', platform.python_version())\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print('Torch:', torch.__version__)\n",
        "    print('CUDA available:', torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU:', torch.cuda.get_device_name(0))\n",
        "except Exception as exc:\n",
        "    print('Torch check failed:', exc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone/update repo and switch into it\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "REPO_ROOT = Path('/content/CL_ml')\n",
        "if not REPO_ROOT.exists():\n",
        "    !git clone https://github.com/epitaph76/CL_ml.git /content/CL_ml\n",
        "else:\n",
        "    print('Repo already exists at', REPO_ROOT)\n",
        "\n",
        "subprocess.run(['git', '-C', str(REPO_ROOT), 'pull', '--ff-only'], check=False)\n",
        "head = subprocess.check_output(['git', '-C', str(REPO_ROOT), 'rev-parse', '--short', 'HEAD'], text=True).strip()\n",
        "print('Repo HEAD:', head)\n",
        "%cd /content/CL_ml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install project dependencies\n",
        "!pip -q install -r /content/CL_ml/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic: make sure repository files are present\n",
        "from pathlib import Path\n",
        "repo = Path('/content/CL_ml')\n",
        "print('Repo exists:', repo.exists())\n",
        "print('src exists:', (repo / 'src').exists())\n",
        "print('moss_tokenize exists:', (repo / 'src' / 'tokenizer' / 'moss_tokenize.py').exists())\n",
        "if repo.exists():\n",
        "    print('Top-level files:', sorted([p.name for p in repo.iterdir()])[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Configure input/output folders\n",
        "\n",
        "Options for audio source (local-first):\n",
        "- Upload a few files directly from your computer\n",
        "- Upload a zip archive with many tracks (recommended for ~1000 files)\n",
        "- Download Navrasa-5000 from HuggingFace\n",
        "- Mix both sources in one corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "REPO_ROOT = Path('/content/CL_ml')\n",
        "if not REPO_ROOT.exists():\n",
        "    raise RuntimeError('Repo root not found. Run clone cell first.')\n",
        "\n",
        "INPUT_ROOT = Path('/content/audio_input')\n",
        "OUTPUT_ROOT = REPO_ROOT / 'data' / 'tokens'\n",
        "SPLITS_ROOT = REPO_ROOT / 'data' / 'splits'\n",
        "\n",
        "INPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "SPLITS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('REPO_ROOT =', REPO_ROOT)\n",
        "print('INPUT_ROOT =', INPUT_ROOT)\n",
        "print('OUTPUT_ROOT =', OUTPUT_ROOT)\n",
        "print('SPLITS_ROOT =', SPLITS_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: upload a few local audio files directly from your computer\n",
        "# For large batches (~1000 files) use the zip upload cell below.\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "uploaded = files.upload()\n",
        "for name in uploaded.keys():\n",
        "    src = Path('/content') / name\n",
        "    dst = INPUT_ROOT / name\n",
        "    if src.exists():\n",
        "        shutil.move(str(src), str(dst))\n",
        "\n",
        "print('Uploaded files:', len(list(INPUT_ROOT.glob('*'))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: upload one or more zip archives from your computer and extract to INPUT_ROOT/custom\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "uploaded = files.upload()\n",
        "zip_names = [name for name in uploaded.keys() if name.lower().endswith('.zip')]\n",
        "\n",
        "if not zip_names:\n",
        "    print('No .zip files uploaded. Skip.')\n",
        "else:\n",
        "    target_root = INPUT_ROOT / 'custom'\n",
        "    target_root.mkdir(parents=True, exist_ok=True)\n",
        "    for name in zip_names:\n",
        "        src = Path('/content') / name\n",
        "        out_dir = target_root / Path(name).stem\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        with zipfile.ZipFile(src, 'r') as zf:\n",
        "            zf.extractall(out_dir)\n",
        "        print('Extracted:', name, '->', out_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optional: download Navrasa-5000 from HuggingFace\n",
        "\n",
        "Set `DOWNLOAD_NAVRASA = True` to download and extract dataset archives into `INPUT_ROOT/navrasa`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import zipfile\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "DOWNLOAD_NAVRASA = False\n",
        "NAVRASA_SUBSETS = ['Global', 'India']   # choose: ['Global'] or ['India'] or both\n",
        "NAVRASA_ZIP_LIMIT = None                # for quick test set a number, e.g. 3\n",
        "HF_DATASET_ID = 'beastLucifer/navrasa-5000-dataset'\n",
        "HF_LOCAL_DIR = Path('/content/navrasa_hf')\n",
        "\n",
        "if DOWNLOAD_NAVRASA:\n",
        "    allow_patterns = [f'{subset}/*.zip' for subset in NAVRASA_SUBSETS] + ['master_manifest.json']\n",
        "    print('Downloading dataset snapshot...')\n",
        "    snapshot_download(\n",
        "        repo_id=HF_DATASET_ID,\n",
        "        repo_type='dataset',\n",
        "        local_dir=str(HF_LOCAL_DIR),\n",
        "        allow_patterns=allow_patterns,\n",
        "        local_dir_use_symlinks=False,\n",
        "    )\n",
        "\n",
        "    zip_files = []\n",
        "    for subset in NAVRASA_SUBSETS:\n",
        "        zip_files.extend(sorted((HF_LOCAL_DIR / subset).glob('*.zip')))\n",
        "    if NAVRASA_ZIP_LIMIT is not None:\n",
        "        zip_files = zip_files[: int(NAVRASA_ZIP_LIMIT)]\n",
        "\n",
        "    target_root = INPUT_ROOT / 'navrasa'\n",
        "    target_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print('Extracting zip archives:', len(zip_files))\n",
        "    for idx, zip_path in enumerate(zip_files, start=1):\n",
        "        out_dir = target_root / zip_path.stem\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "            zf.extractall(out_dir)\n",
        "        if idx % 10 == 0 or idx == len(zip_files):\n",
        "            print(f'  extracted {idx}/{len(zip_files)}: {zip_path.name}')\n",
        "\n",
        "    print('Navrasa extracted to:', target_root)\n",
        "else:\n",
        "    print('Skip Navrasa download. Set DOWNLOAD_NAVRASA=True to enable.')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Inspect final audio corpus before tokenization\n",
        "from collections import Counter\n",
        "\n",
        "exts = {'.mp3', '.wav', '.flac', '.ogg', '.m4a'}\n",
        "audio_files = [p for p in INPUT_ROOT.rglob('*') if p.is_file() and p.suffix.lower() in exts]\n",
        "counts = Counter(p.suffix.lower() for p in audio_files)\n",
        "\n",
        "print('INPUT_ROOT:', INPUT_ROOT)\n",
        "print('Total audio files:', len(audio_files))\n",
        "print('By extension:', dict(sorted(counts.items())))\n",
        "for p in audio_files[:10]:\n",
        "    print('-', p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Run MOSS tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full run with detailed logging (absolute script path)\n",
        "import shlex\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "script_path = REPO_ROOT / 'src' / 'tokenizer' / 'moss_tokenize.py'\n",
        "if not script_path.exists():\n",
        "    raise RuntimeError(f'Script not found: {script_path}. Re-run clone cell.')\n",
        "\n",
        "exts = {'.mp3', '.wav', '.flac', '.ogg', '.m4a'}\n",
        "if INPUT_ROOT.is_file():\n",
        "    found = [INPUT_ROOT]\n",
        "else:\n",
        "    found = sorted([p for p in INPUT_ROOT.rglob('*') if p.is_file() and p.suffix.lower() in exts])\n",
        "\n",
        "print('Found audio files:', len(found))\n",
        "for p in found[:10]:\n",
        "    print('-', p)\n",
        "if not found:\n",
        "    raise RuntimeError(f'No audio files found under: {INPUT_ROOT}')\n",
        "\n",
        "cmd = [\n",
        "    'python', str(script_path),\n",
        "    '--input-root', str(INPUT_ROOT),\n",
        "    '--output-root', str(OUTPUT_ROOT),\n",
        "    '--device', 'auto',\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
        "env = os.environ.copy()\n",
        "env['PYTHONPATH'] = str(REPO_ROOT) + (':' + env['PYTHONPATH'] if env.get('PYTHONPATH') else '')\n",
        "result = subprocess.run(cmd, cwd=str(REPO_ROOT), env=env, text=True, capture_output=True)\n",
        "print('\\n--- STDOUT ---')\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print('--- STDERR ---')\n",
        "    print(result.stderr)\n",
        "    raise RuntimeError(f'moss_tokenize failed with code {result.returncode}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smoke run on a subset (uncomment and run if needed)\n",
        "# import shlex, subprocess, os\n",
        "# script_path = REPO_ROOT / 'src' / 'tokenizer' / 'moss_tokenize.py'\n",
        "# cmd = [\n",
        "#     'python', str(script_path),\n",
        "#     '--input-root', str(INPUT_ROOT),\n",
        "#     '--output-root', str(OUTPUT_ROOT),\n",
        "#     '--device', 'auto',\n",
        "#     '--max-files', '10',\n",
        "# ]\n",
        "# print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
        "# env = os.environ.copy()\n",
        "# env['PYTHONPATH'] = str(REPO_ROOT) + (':' + env['PYTHONPATH'] if env.get('PYTHONPATH') else '')\n",
        "# subprocess.run(cmd, cwd=str(REPO_ROOT), env=env, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Inspect token outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "token_files = sorted(Path(OUTPUT_ROOT).glob('*.pt')) + sorted(Path(OUTPUT_ROOT).glob('*.npz'))\n",
        "print('Token files:', len(token_files))\n",
        "for p in token_files[:5]:\n",
        "    print('-', p.name)\n",
        "\n",
        "if token_files and token_files[0].suffix == '.pt':\n",
        "    sample = torch.load(token_files[0], map_location='cpu')\n",
        "    print('sample track_id:', sample.get('track_id'))\n",
        "    print('sample token_shape:', sample.get('token_shape'))\n",
        "    print('tensor shape:', tuple(sample['tokens'].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build split files\n",
        "\n",
        "Run the next code cell to generate `train/val/test` split files.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shlex\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "script_path = REPO_ROOT / 'src' / 'dataset' / 'build_splits.py'\n",
        "if not script_path.exists():\n",
        "    raise RuntimeError(f'Script not found: {script_path}. Re-run clone cell.')\n",
        "\n",
        "cmd = [\n",
        "    'python', str(script_path),\n",
        "    '--tokens-root', str(OUTPUT_ROOT),\n",
        "    '--output-root', str(SPLITS_ROOT),\n",
        "    '--val-ratio', '0.1',\n",
        "    '--test-ratio', '0.1',\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
        "env = os.environ.copy()\n",
        "env['PYTHONPATH'] = str(REPO_ROOT) + (':' + env['PYTHONPATH'] if env.get('PYTHONPATH') else '')\n",
        "result = subprocess.run(cmd, cwd=str(REPO_ROOT), env=env, text=True, capture_output=True)\n",
        "print('\\n--- STDOUT ---')\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print('--- STDERR ---')\n",
        "    print(result.stderr)\n",
        "    raise RuntimeError(f'build_splits failed with code {result.returncode}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name in ['train.txt', 'val.txt', 'test.txt', 'summary.json']:\n",
        "    p = SPLITS_ROOT / name\n",
        "    print('\\\\n===', name, '===')\n",
        "    if p.exists():\n",
        "        print(p.read_text(encoding='utf-8')[:500])\n",
        "    else:\n",
        "        print('not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Train and evaluate retrieval\n",
        "\n",
        "Continue in this same notebook: training + offline retrieval metrics.\n",
        "\n",
        "Default eval below uses `--eval-protocol cross_chunk --exclude-self` (more honest).\n",
        "It needs at least 2 chunks per track, so tokenization should use `--chunk-seconds`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "CHECKPOINTS_ROOT = REPO_ROOT / 'data' / 'checkpoints'\n",
        "REPORTS_ROOT = REPO_ROOT / 'data' / 'reports'\n",
        "CHECKPOINTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "REPORTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('OUTPUT_ROOT =', OUTPUT_ROOT)\n",
        "print('SPLITS_ROOT =', SPLITS_ROOT)\n",
        "print('CHECKPOINTS_ROOT =', CHECKPOINTS_ROOT)\n",
        "print('REPORTS_ROOT =', REPORTS_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Full training run\n",
        "import shlex\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "cmd = [\n",
        "    'python', '-m', 'src.train.train_contrastive',\n",
        "    '--config', 'configs/train.yaml',\n",
        "    '--device', 'auto',\n",
        "    '--output-dir', str(CHECKPOINTS_ROOT),\n",
        "    '--batch-size', '4',\n",
        "    '--num-workers', '2',\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
        "env = os.environ.copy()\n",
        "env['PYTHONPATH'] = str(REPO_ROOT) + (':' + env['PYTHONPATH'] if env.get('PYTHONPATH') else '')\n",
        "result = subprocess.run(cmd, cwd=str(REPO_ROOT), env=env, text=True, capture_output=True)\n",
        "print('\\n--- STDOUT ---')\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print('--- STDERR ---')\n",
        "    print(result.stderr)\n",
        "    raise RuntimeError(f'train_contrastive failed with code {result.returncode}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Smoke training run (optional)\n",
        "# import shlex, subprocess, os\n",
        "# cmd = [\n",
        "#     'python', '-m', 'src.train.train_contrastive',\n",
        "#     '--config', 'configs/train.yaml',\n",
        "#     '--device', 'auto',\n",
        "#     '--max-steps-per-epoch', '5',\n",
        "#     '--output-dir', str(REPO_ROOT / 'data' / 'checkpoints_smoke'),\n",
        "# ]\n",
        "# print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
        "# env = os.environ.copy()\n",
        "# env['PYTHONPATH'] = str(REPO_ROOT) + (':' + env['PYTHONPATH'] if env.get('PYTHONPATH') else '')\n",
        "# subprocess.run(cmd, cwd=str(REPO_ROOT), env=env, check=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "for name in ['best.pt', 'last.pt', 'history.json']:\n",
        "    p = CHECKPOINTS_ROOT / name\n",
        "    print(name, '->', 'exists' if p.exists() else 'missing', p)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Exact retrieval metrics\n",
        "import shlex\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "cmd = [\n",
        "    'python', '-m', 'src.index.evaluate_retrieval',\n",
        "    '--config', 'configs/train.yaml',\n",
        "    '--checkpoint', str(CHECKPOINTS_ROOT / 'best.pt'),\n",
        "    '--tokens-root', str(OUTPUT_ROOT),\n",
        "    '--splits-root', str(SPLITS_ROOT),\n",
        "    '--split', 'val',\n",
        "    '--topk', '1,10,100',\n",
        "    '--device', 'auto',\n",
        "    '--eval-protocol', 'cross_chunk',\n",
        "    '--exclude-self',\n",
        "    '--batch-size', '16',\n",
        "    '--output-json', str(REPORTS_ROOT / 'val_exact.json'),\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
        "env = os.environ.copy()\n",
        "env['PYTHONPATH'] = str(REPO_ROOT) + (':' + env['PYTHONPATH'] if env.get('PYTHONPATH') else '')\n",
        "result = subprocess.run(cmd, cwd=str(REPO_ROOT), env=env, text=True, capture_output=True)\n",
        "print('\\n--- STDOUT ---')\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print('--- STDERR ---')\n",
        "    print(result.stderr)\n",
        "    raise RuntimeError(f'evaluate_retrieval failed with code {result.returncode}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Exact + FAISS metrics (optional)\n",
        "import shlex\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "cmd = [\n",
        "    'python', '-m', 'src.index.evaluate_retrieval',\n",
        "    '--config', 'configs/train.yaml',\n",
        "    '--checkpoint', str(CHECKPOINTS_ROOT / 'best.pt'),\n",
        "    '--tokens-root', str(OUTPUT_ROOT),\n",
        "    '--splits-root', str(SPLITS_ROOT),\n",
        "    '--split', 'val',\n",
        "    '--topk', '1,10,100',\n",
        "    '--device', 'auto',\n",
        "    '--eval-protocol', 'cross_chunk',\n",
        "    '--exclude-self',\n",
        "    '--batch-size', '16',\n",
        "    '--use-faiss',\n",
        "    '--output-json', str(REPORTS_ROOT / 'val_exact_faiss.json'),\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
        "env = os.environ.copy()\n",
        "env['PYTHONPATH'] = str(REPO_ROOT) + (':' + env['PYTHONPATH'] if env.get('PYTHONPATH') else '')\n",
        "result = subprocess.run(cmd, cwd=str(REPO_ROOT), env=env, text=True, capture_output=True)\n",
        "print('\\n--- STDOUT ---')\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print('--- STDERR ---')\n",
        "    print(result.stderr)\n",
        "    raise RuntimeError(f'evaluate_retrieval (faiss) failed with code {result.returncode}')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "for name in ['val_exact.json', 'val_exact_faiss.json']:\n",
        "    p = REPORTS_ROOT / name\n",
        "    print('\\n===', name, '===')\n",
        "    if p.exists():\n",
        "        payload = json.loads(p.read_text(encoding='utf-8'))\n",
        "        print(json.dumps(payload.get('results', {}), indent=2, ensure_ascii=False))\n",
        "    else:\n",
        "        print('not found')\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional: save artifacts to Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# !mkdir -p /content/drive/MyDrive/CL_ml_runs/run1\n",
        "# !cp -r /content/CL_ml/data/checkpoints /content/drive/MyDrive/CL_ml_runs/run1/\n",
        "# !cp -r /content/CL_ml/data/reports /content/drive/MyDrive/CL_ml_runs/run1/\n",
        "# !cp -r /content/CL_ml/configs /content/drive/MyDrive/CL_ml_runs/run1/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Done\n",
        "\n",
        "You now have one Colab notebook for end-to-end baseline: `audio -> tokens -> splits -> train -> retrieval metrics`.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "01_moss_tokenization_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}