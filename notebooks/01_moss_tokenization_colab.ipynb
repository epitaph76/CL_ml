{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MOSS Tokenization in Colab\n",
        "\n",
        "This notebook converts audio files (`mp3/wav/flac/...`) into discrete MOSS tokens for retrieval training.\n",
        "\n",
        "Pipeline in this notebook:\n",
        "1. Clone project repo\n",
        "2. Install dependencies\n",
        "3. Upload audio files (or use Google Drive path)\n",
        "4. Run tokenization (`audio -> tokens`)\n",
        "5. Validate output token files\n",
        "6. Build train/val/test split files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Runtime\n",
        "\n",
        "In Colab: `Runtime -> Change runtime type -> GPU` (optional but recommended)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform\n",
        "print('Python:', platform.python_version())\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print('Torch:', torch.__version__)\n",
        "    print('CUDA available:', torch.cuda.is_available())\n",
        "    if torch.cuda.is_available():\n",
        "        print('GPU:', torch.cuda.get_device_name(0))\n",
        "except Exception as exc:\n",
        "    print('Torch check failed:', exc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone your repo and switch into it\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "REPO_ROOT = Path('/content/CL_ml')\n",
        "if not REPO_ROOT.exists():\n",
        "    !git clone https://github.com/epitaph76/CL_ml.git /content/CL_ml\n",
        "else:\n",
        "    print('Repo already exists at', REPO_ROOT)\n",
        "\n",
        "%cd /content/CL_ml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install project dependencies\n",
        "!pip -q install -r /content/CL_ml/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diagnostic: make sure repository files are present\n",
        "from pathlib import Path\n",
        "repo = Path('/content/CL_ml')\n",
        "print('Repo exists:', repo.exists())\n",
        "print('src exists:', (repo / 'src').exists())\n",
        "print('moss_tokenize exists:', (repo / 'src' / 'tokenizer' / 'moss_tokenize.py').exists())\n",
        "if repo.exists():\n",
        "    print('Top-level files:', sorted([p.name for p in repo.iterdir()])[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Configure input/output folders\n",
        "\n",
        "Option A (simple): upload files to `/content/audio_input`.\n",
        "Option B: use Google Drive folder path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "REPO_ROOT = Path('/content/CL_ml')\n",
        "if not REPO_ROOT.exists():\n",
        "    raise RuntimeError('Repo root not found. Run clone cell first.')\n",
        "\n",
        "INPUT_ROOT = Path('/content/audio_input')\n",
        "OUTPUT_ROOT = REPO_ROOT / 'data' / 'tokens'\n",
        "SPLITS_ROOT = REPO_ROOT / 'data' / 'splits'\n",
        "\n",
        "INPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "SPLITS_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('REPO_ROOT =', REPO_ROOT)\n",
        "print('INPUT_ROOT =', INPUT_ROOT)\n",
        "print('OUTPUT_ROOT =', OUTPUT_ROOT)\n",
        "print('SPLITS_ROOT =', SPLITS_ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: upload local audio files directly to Colab\n",
        "# Skip this cell if files are already in INPUT_ROOT or on Drive.\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "uploaded = files.upload()\n",
        "for name in uploaded.keys():\n",
        "    src = Path('/content') / name\n",
        "    dst = INPUT_ROOT / name\n",
        "    if src.exists():\n",
        "        shutil.move(str(src), str(dst))\n",
        "\n",
        "print('Uploaded files:', len(list(INPUT_ROOT.glob('*'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: mount Google Drive and use a Drive folder as input\n",
        "# Example:\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# INPUT_ROOT = Path('/content/drive/MyDrive/your_audio_folder')\n",
        "# print('INPUT_ROOT switched to', INPUT_ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Run MOSS tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full run with detailed logging (absolute script path)\n",
        "import shlex\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "script_path = REPO_ROOT / 'src' / 'tokenizer' / 'moss_tokenize.py'\n",
        "if not script_path.exists():\n",
        "    raise RuntimeError(f'Script not found: {script_path}. Re-run clone cell.')\n",
        "\n",
        "exts = {'.mp3', '.wav', '.flac', '.ogg', '.m4a'}\n",
        "if INPUT_ROOT.is_file():\n",
        "    found = [INPUT_ROOT]\n",
        "else:\n",
        "    found = sorted([p for p in INPUT_ROOT.rglob('*') if p.is_file() and p.suffix.lower() in exts])\n",
        "\n",
        "print('Found audio files:', len(found))\n",
        "for p in found[:10]:\n",
        "    print('-', p)\n",
        "if not found:\n",
        "    raise RuntimeError(f'No audio files found under: {INPUT_ROOT}')\n",
        "\n",
        "cmd = [\n",
        "    'python', str(script_path),\n",
        "    '--input-root', str(INPUT_ROOT),\n",
        "    '--output-root', str(OUTPUT_ROOT),\n",
        "    '--device', 'auto',\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
        "env = os.environ.copy()\n",
        "env['PYTHONPATH'] = str(REPO_ROOT) + (':' + env['PYTHONPATH'] if env.get('PYTHONPATH') else '')\n",
        "result = subprocess.run(cmd, cwd=str(REPO_ROOT), env=env, text=True, capture_output=True)\n",
        "print('\\n--- STDOUT ---')\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print('--- STDERR ---')\n",
        "    print(result.stderr)\n",
        "    raise RuntimeError(f'moss_tokenize failed with code {result.returncode}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Smoke run on a subset (uncomment and run if needed)\n",
        "# import shlex, subprocess, os\n",
        "# script_path = REPO_ROOT / 'src' / 'tokenizer' / 'moss_tokenize.py'\n",
        "# cmd = [\n",
        "#     'python', str(script_path),\n",
        "#     '--input-root', str(INPUT_ROOT),\n",
        "#     '--output-root', str(OUTPUT_ROOT),\n",
        "#     '--device', 'auto',\n",
        "#     '--max-files', '10',\n",
        "# ]\n",
        "# print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
        "# env = os.environ.copy()\n",
        "# env['PYTHONPATH'] = str(REPO_ROOT) + (':' + env['PYTHONPATH'] if env.get('PYTHONPATH') else '')\n",
        "# subprocess.run(cmd, cwd=str(REPO_ROOT), env=env, check=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Inspect token outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "token_files = sorted(Path(OUTPUT_ROOT).glob('*.pt')) + sorted(Path(OUTPUT_ROOT).glob('*.npz'))\n",
        "print('Token files:', len(token_files))\n",
        "for p in token_files[:5]:\n",
        "    print('-', p.name)\n",
        "\n",
        "if token_files and token_files[0].suffix == '.pt':\n",
        "    sample = torch.load(token_files[0], map_location='cpu')\n",
        "    print('sample track_id:', sample.get('track_id'))\n",
        "    print('sample token_shape:', sample.get('token_shape'))\n",
        "    print('tensor shape:', tuple(sample['tokens'].shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import shlex\n",
        "import subprocess\n",
        "\n",
        "cmd = [\n",
        "    'python', '-m', 'src.dataset.build_splits',\n",
        "    '--tokens-root', str(OUTPUT_ROOT),\n",
        "    '--output-root', str(SPLITS_ROOT),\n",
        "    '--val-ratio', '0.1',\n",
        "    '--test-ratio', '0.1',\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shlex\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "script_path = REPO_ROOT / 'src' / 'dataset' / 'build_splits.py'\n",
        "if not script_path.exists():\n",
        "    raise RuntimeError(f'Script not found: {script_path}. Re-run clone cell.')\n",
        "\n",
        "cmd = [\n",
        "    'python', str(script_path),\n",
        "    '--tokens-root', str(OUTPUT_ROOT),\n",
        "    '--output-root', str(SPLITS_ROOT),\n",
        "    '--val-ratio', '0.1',\n",
        "    '--test-ratio', '0.1',\n",
        "]\n",
        "print('Running:', ' '.join(shlex.quote(x) for x in cmd))\n",
        "env = os.environ.copy()\n",
        "env['PYTHONPATH'] = str(REPO_ROOT) + (':' + env['PYTHONPATH'] if env.get('PYTHONPATH') else '')\n",
        "result = subprocess.run(cmd, cwd=str(REPO_ROOT), env=env, text=True, capture_output=True)\n",
        "print('\\n--- STDOUT ---')\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print('--- STDERR ---')\n",
        "    print(result.stderr)\n",
        "    raise RuntimeError(f'build_splits failed with code {result.returncode}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name in ['train.txt', 'val.txt', 'test.txt', 'summary.json']:\n",
        "    p = SPLITS_ROOT / name\n",
        "    print('\\\\n===', name, '===')\n",
        "    if p.exists():\n",
        "        print(p.read_text(encoding='utf-8')[:500])\n",
        "    else:\n",
        "        print('not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next\n",
        "\n",
        "After this notebook, move to model training: `TokenPairDataset -> embedder -> contrastive loss`."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "01_moss_tokenization_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}